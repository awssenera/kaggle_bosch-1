{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Bosch Production Line Challenge Model\n",
    "## Summary\n",
    "This is a machine learning model, built on a [Spark](https://spark.apache.org) framework, to attempt to solve the [Bosch Production Line Performance Challenge](https://www.kaggle.com/c/bosch-production-line-performance) on [Kaggle](https://www.kaggle.com).  This project was begun by Thomas Hughes on November 24, 2016, after the competition was completed.  It should be considered a test of effectiveness of technology platforms.\n",
    "\n",
    "## Notes on Execution\n",
    "Since this Notebook is designed to run with Spark, it must be running with the PySpark interpreter.  This can be done mostly automatically if you launch the notebook using the script 'pyspark-notebook' that is available in the github repository along with the notebook.  PySpark will need to be installed and properly configured, and you may need to update the script to your local copy of PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading settings...\n",
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load File Locations, using Kaggle specifications\n",
    "import json\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "print \"Loading settings...\"\n",
    "with open('SETTINGS.json') as settings_file:\n",
    "    settings = json.load(settings_file)\n",
    "\n",
    "print \"Loaded!\"\n",
    "\n",
    "# Source directory for your data\n",
    "source_dir = settings['source_dir']\n",
    "\n",
    "# sc is the SparkContext provided by the pyspark interpreter.  That's why you don't see it initialized here.\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Bosch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "def data_import(t):\n",
    "    \"\"\" Helper function that loads the data files from the disk and does some pre-processing before they can be sent\n",
    "    to the pipeline.  Would be ideal if this could be done in the pipeline, but several of these functions only work\n",
    "    a single column at a time.\"\"\"\n",
    "    \n",
    "    if t == 'train':\n",
    "        numeric_file = settings['train_numeric_file']\n",
    "        categorical_file = settings['train_categorical_file']\n",
    "    elif t == 'test':\n",
    "        numeric_file = settings['test_numeric_file']\n",
    "        categorical_file = settings['test_categorical_file']\n",
    "    else:\n",
    "        return 'Error, data_import can only take strings \"train\" or \"test\"'\n",
    "    \n",
    "    # Import Bosch training numeric data\n",
    "    print \"Loading numeric data...\"\n",
    "    source_numeric = source_dir + numeric_file\n",
    "    numeric = sqlContext.read.csv(source_numeric, header = \"true\", inferSchema = \"true\")\n",
    "    \n",
    "    print \"Filling missing values...\"\n",
    "    # Fill remaining missing values with 0.\n",
    "    numeric = numeric.na.fill(0)\n",
    "    \n",
    "    print \"Loading categorical data...\"\n",
    "    # Now the categorical data\n",
    "    source_categorical = source_dir + categorical_file\n",
    "    categorical = sqlContext.read.csv(source_categorical, header=\"true\", inferSchema=\"true\")\n",
    "\n",
    "    # Sample, for preliminary testing\n",
    "    if t == 'train':\n",
    "        print 'Selecting sample data...'\n",
    "        categorical = categorical.sample(withReplacement = False, fraction=0.01, seed=42)    \n",
    "    \n",
    "    print \"Dropping missing values...\"\n",
    "    # Drop columns that contain no data\n",
    "    drop_list = ['L0_S3_F69', 'L0_S3_F71', 'L0_S3_F73', 'L0_S3_F75', 'L0_S3_F77', 'L0_S3_F79', 'L0_S3_F81', \n",
    "                 'L0_S3_F83', 'L0_S3_F85', 'L0_S3_F87', 'L0_S3_F89', 'L0_S3_F91', 'L0_S3_F93', 'L0_S3_F95', \n",
    "                 'L0_S3_F97', 'L0_S3_F99', 'L0_S3_F101', 'L0_S3_F103', 'L0_S18_F436', 'L0_S18_F438', 'L0_S18_F440', \n",
    "                 'L0_S18_F442', 'L0_S18_F443', 'L0_S18_F445', 'L0_S18_F446', 'L0_S18_F448', 'L0_S18_F450', \n",
    "                 'L0_S18_F452', 'L0_S23_F616', 'L0_S23_F618', 'L0_S23_F620', 'L0_S23_F622', 'L0_S23_F624', \n",
    "                 'L0_S23_F626', 'L0_S23_F628', 'L0_S23_F630', 'L0_S23_F632', 'L0_S23_F634', 'L0_S23_F636', \n",
    "                 'L0_S23_F638', 'L0_S23_F640', 'L0_S23_F642', 'L0_S23_F644', 'L0_S23_F646', 'L0_S23_F648', \n",
    "                 'L0_S23_F650', 'L0_S23_F652', 'L0_S23_F654', 'L0_S23_F656', 'L0_S23_F658', 'L0_S23_F660', \n",
    "                 'L0_S23_F662', 'L0_S23_F664', 'L0_S23_F666', 'L0_S23_F668', 'L0_S23_F670', 'L0_S23_F672', \n",
    "                 'L0_S23_F674', 'L1_S24_F676', 'L1_S24_F678', 'L1_S24_F680', 'L1_S24_F682', 'L1_S24_F684', \n",
    "                 'L1_S24_F686', 'L1_S24_F688', 'L1_S24_F690', 'L1_S24_F692', 'L1_S24_F694', 'L1_S24_F1157', \n",
    "                 'L1_S24_F1159', 'L1_S24_F1160', 'L1_S24_F1167', 'L1_S24_F1169', 'L1_S24_F1177', 'L1_S24_F1179', \n",
    "                 'L1_S24_F1181', 'L1_S24_F1183', 'L1_S24_F1561', 'L1_S24_F1563', 'L1_S24_F1564', 'L1_S24_F1673', \n",
    "                 'L1_S24_F1676', 'L1_S24_F1677', 'L1_S24_F1680', 'L1_S24_F1681', 'L1_S24_F1684', 'L1_S24_F1686', \n",
    "                 'L1_S24_F1689', 'L1_S24_F1691', 'L1_S24_F1694', 'L1_S24_F1696', 'L1_S24_F1699', 'L1_S24_F1701', \n",
    "                 'L1_S24_F1704', 'L1_S24_F1705', 'L1_S24_F1708', 'L1_S24_F1709', 'L1_S24_F1712', 'L1_S24_F1714', \n",
    "                 'L1_S24_F1717', 'L1_S24_F1719', 'L1_S24_F1722', 'L1_S24_F1724', 'L1_S24_F1727', 'L1_S24_F1729', \n",
    "                 'L1_S24_F1732', 'L1_S24_F1734', 'L1_S24_F1737', 'L1_S24_F1739', 'L1_S24_F1742', 'L1_S24_F1744', \n",
    "                 'L1_S24_F1747', 'L1_S24_F1749', 'L1_S24_F1752', 'L1_S24_F1754', 'L1_S24_F1757', 'L1_S24_F1759', \n",
    "                 'L1_S24_F1762', 'L1_S25_F1853', 'L1_S25_F1856', 'L1_S25_F1859', 'L1_S25_F1861', 'L1_S25_F1863', \n",
    "                 'L1_S25_F2956', 'L1_S25_F2959', 'L1_S25_F2961', 'L1_S25_F2964', 'L1_S25_F2966', 'L1_S25_F2969', \n",
    "                 'L1_S25_F2971', 'L1_S25_F2974', 'L1_S25_F2976', 'L1_S25_F2979', 'L1_S25_F2981', 'L1_S25_F2984', \n",
    "                 'L1_S25_F2986', 'L1_S25_F2989', 'L1_S25_F2991', 'L1_S25_F2994', 'L3_S46_F4136', 'L3_S46_F4137', \n",
    "                 'L3_S47_F4139', 'L3_S47_F4142', 'L3_S47_F4144', 'L3_S47_F4147', 'L3_S47_F4149', 'L3_S47_F4152', \n",
    "                 'L3_S47_F4154', 'L3_S47_F4157', 'L3_S47_F4159', 'L3_S47_F4162', 'L3_S47_F4164', 'L3_S47_F4167', \n",
    "                 'L3_S47_F4169', 'L3_S47_F4172', 'L3_S47_F4174', 'L3_S47_F4177', 'L3_S47_F4179', 'L3_S47_F4182', \n",
    "                 'L3_S47_F4184', 'L3_S47_F4187', 'L3_S47_F4189', 'L3_S47_F4192']\n",
    "    \n",
    "    \n",
    "    good_columns = [x for x in categorical.columns if x not in drop_list]\n",
    "    \n",
    "    categorical = categorical.select(good_columns)\n",
    "    \n",
    "    # Fill remaining missing values with 'none' category; string needed for transformers\n",
    "    print \"Filling remaining missing values...\"\n",
    "    categorical = categorical.na.fill('None')\n",
    "    \n",
    "    # This totally feels messy, but there does not seem to be a way to convert multiple categorical \n",
    "    #strings into one hots from the ml pipeline framework.  Hopefully that is corrected in the future.\n",
    "    print \"One Hot encoding categorical data... (patience, this takes a while)\"\n",
    "    ignore = ['Id', 'Response']\n",
    "    categorical_columns = [x for x in categorical.columns if x not in ignore]\n",
    "   \n",
    "    indexed_df = categorical\n",
    "    encoded_df = categorical\n",
    "    \n",
    "    drop_index = []\n",
    "    \n",
    "    # This goes through all the remaining categorical columns, converts them to indexes, then one-hot vectors\n",
    "    for col in categorical_columns:\n",
    "        indexer = StringIndexer(inputCol=col, outputCol=(col+\"_indexed\")).fit(encoded_df)\n",
    "        indexed_df = indexer.transform(encoded_df)\n",
    "     #   encoded_df = OneHotEncoder(inputCol=(col+\"_indexed\"), outputCol=(col+\"_vector\")).transform(indexed_df)\n",
    "        drop_index.append(col)\n",
    "    \n",
    "    vectored_columns = [x for x in indexer_df.columns if x not in drop_index]\n",
    "    categorical = index_df.select(vectored_columns)\n",
    "    \n",
    "    print \"Joining numeric and categorical data...\"\n",
    "    # Combine the numeric with the categorical dataframe, right_outer works for sampling\n",
    "    df = numeric.join(categorical, on='Id', how='right_outer')\n",
    "    \n",
    "    print \"Data import complete!\"\n",
    "    \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load training data...\n",
      "Loading numeric data...\n",
      "Filling missing values...\n",
      "Loading categorical data...\n",
      "Selecting sample data...\n",
      "Dropping missing values...\n",
      "Filling remaining missing values...\n",
      "One Hot encoding categorical data... (patience, this takes a while)\n"
     ]
    }
   ],
   "source": [
    "print \"Load training data...\"\n",
    "train = data_import(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building VectorAssembler...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-698796039374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Only vectorize the non-ID and non-Response columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mignore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m assembler = VectorAssembler(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# We need to vectorize our features for MLLib\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "print \"Building VectorAssembler...\"\n",
    "# Only vectorize the non-ID and non-Response columns\n",
    "ignore = ['Id', 'Response']\n",
    "train_columns = [x for x in train.columns if x not in ignore]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = train_columns,\n",
    "    outputCol = 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Train a GBT model.\n",
    "print \"Building GBTClassifier...\"\n",
    "gbt = GBTClassifier(labelCol = \"Response\", featuresCol = \"features\", maxIter = 10, maxDepth = 10, \n",
    "                    maxMemoryInMB = 1024, maxBins = 64)\n",
    "\n",
    "print \"Building Pipeline...\"\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages = [assembler, gbt])\n",
    "\n",
    "print \"Fitting data to model...\"\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "print \"Model generated!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Serialize the Model\n",
    "print \"Saving model to disk...\"\n",
    "\n",
    "model_source = source_dir + settings['model_file']\n",
    "model.save(model_source)\n",
    "\n",
    "print \"Model saved!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance\n",
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Load test data...\"\n",
    "test = load_data('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Load serialized model\n",
    "print \"Load model...\"\n",
    "\n",
    "model = PipelineModel([]).load(source_dir + settings['model_file'])\n",
    "\n",
    "print \"Model loaded!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "print \"Making predictions for test data...\"\n",
    "\n",
    "preds = model.transform(test)\n",
    "\n",
    "print \"Predictions complete!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format and Export Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Collect the prediction from Spark\n",
    "print \"Formatting and saving Kaggle submission...\"\n",
    "predsGBT = preds.select(\"prediction\").rdd.map(lambda r: r[0]).collect()\n",
    "\n",
    "# Format to Kaggle Format\n",
    "sub = pd.read_csv(source_dir + settings['sample_submission_file'])\n",
    "sub['Response'] = np.asarray(predsGBT).astype(int)\n",
    "sub.to_csv(source_dir + settings['final_submission_file'], index = False)\n",
    "\n",
    "print \"Submission complete!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission History\n",
    "\n",
    "* Submission 1: -\t\tThomas M Hughes\t0.13591\t-\tSun, 27 Nov 2016 23:03:33 (GBT)\n",
    "* Submission 2: -\t\tThomas M Hughes\t0.13591\t-\tMon, 28 Nov 2016 00:06:04 (GBT w/ Standard Scaler)\n",
    "* Submission 3: -\t\tThomas M Hughes\t0.15070\t-\tMon, 28 Nov 2016 01:27:14 (GBT w/ maxDepth=10, maxBins=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsolete Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Bosch training numeric data\n",
    "source_numeric = source_dir + settings['train_numeric_file']\n",
    "train_numeric = sqlContext.read.csv(source_numeric, header = \"true\", inferSchema = \"true\")\n",
    "\n",
    "# Fill missing values with 0.\n",
    "#train_numeric = train_numeric.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# Now the categorical data\n",
    "source_categorical = source_dir + settings['train_categorical_file']\n",
    "train_categorical = sqlContext.read.csv(source_categorical, header=\"true\", inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ignore = ['Id', 'Response']\n",
    "numeric_columns = [x for x in train_numeric.columns if x not in ignore]\n",
    "\n",
    "drop_list = []\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if train_numeric.filter(train_numeric[col] != None).count() == 0:\n",
    "        drop_list.append(col)\n",
    "        print col\n",
    "\n",
    "            \n",
    "print drop_list\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following columns have no variation, and thus contain no useful information\n",
    "drop_list = ['L0_S3_F69', 'L0_S3_F71', 'L0_S3_F73', 'L0_S3_F75', 'L0_S3_F77', 'L0_S3_F79', 'L0_S3_F81', \n",
    "             'L0_S3_F83', 'L0_S3_F85', 'L0_S3_F87', 'L0_S3_F89', 'L0_S3_F91', 'L0_S3_F93', 'L0_S3_F95', \n",
    "             'L0_S3_F97', 'L0_S3_F99', 'L0_S3_F101', 'L0_S3_F103', 'L0_S18_F436', 'L0_S18_F438', 'L0_S18_F440', \n",
    "             'L0_S18_F442', 'L0_S18_F443', 'L0_S18_F445', 'L0_S18_F446', 'L0_S18_F448', 'L0_S18_F450', \n",
    "             'L0_S18_F452', 'L0_S23_F616', 'L0_S23_F618', 'L0_S23_F620', 'L0_S23_F622', 'L0_S23_F624', \n",
    "             'L0_S23_F626', 'L0_S23_F628', 'L0_S23_F630', 'L0_S23_F632', 'L0_S23_F634', 'L0_S23_F636', \n",
    "             'L0_S23_F638', 'L0_S23_F640', 'L0_S23_F642', 'L0_S23_F644', 'L0_S23_F646', 'L0_S23_F648', \n",
    "             'L0_S23_F650', 'L0_S23_F652', 'L0_S23_F654', 'L0_S23_F656', 'L0_S23_F658', 'L0_S23_F660', \n",
    "             'L0_S23_F662', 'L0_S23_F664', 'L0_S23_F666', 'L0_S23_F668', 'L0_S23_F670', 'L0_S23_F672', \n",
    "             'L0_S23_F674', 'L1_S24_F676', 'L1_S24_F678', 'L1_S24_F680', 'L1_S24_F682', 'L1_S24_F684', \n",
    "             'L1_S24_F686', 'L1_S24_F688', 'L1_S24_F690', 'L1_S24_F692', 'L1_S24_F694', 'L1_S24_F1157', \n",
    "             'L1_S24_F1159', 'L1_S24_F1160', 'L1_S24_F1167', 'L1_S24_F1169', 'L1_S24_F1177', 'L1_S24_F1179', \n",
    "             'L1_S24_F1181', 'L1_S24_F1183', 'L1_S24_F1561', 'L1_S24_F1563', 'L1_S24_F1564', 'L1_S24_F1673', \n",
    "             'L1_S24_F1676', 'L1_S24_F1677', 'L1_S24_F1680', 'L1_S24_F1681', 'L1_S24_F1684', 'L1_S24_F1686', \n",
    "             'L1_S24_F1689', 'L1_S24_F1691', 'L1_S24_F1694', 'L1_S24_F1696', 'L1_S24_F1699', 'L1_S24_F1701', \n",
    "             'L1_S24_F1704', 'L1_S24_F1705', 'L1_S24_F1708', 'L1_S24_F1709', 'L1_S24_F1712', 'L1_S24_F1714', \n",
    "             'L1_S24_F1717', 'L1_S24_F1719', 'L1_S24_F1722', 'L1_S24_F1724', 'L1_S24_F1727', 'L1_S24_F1729', \n",
    "             'L1_S24_F1732', 'L1_S24_F1734', 'L1_S24_F1737', 'L1_S24_F1739', 'L1_S24_F1742', 'L1_S24_F1744', \n",
    "             'L1_S24_F1747', 'L1_S24_F1749', 'L1_S24_F1752', 'L1_S24_F1754', 'L1_S24_F1757', 'L1_S24_F1759', \n",
    "             'L1_S24_F1762', 'L1_S25_F1853', 'L1_S25_F1856', 'L1_S25_F1859', 'L1_S25_F1861', 'L1_S25_F1863', \n",
    "             'L1_S25_F2956', 'L1_S25_F2959', 'L1_S25_F2961', 'L1_S25_F2964', 'L1_S25_F2966', 'L1_S25_F2969', \n",
    "             'L1_S25_F2971', 'L1_S25_F2974', 'L1_S25_F2976', 'L1_S25_F2979', 'L1_S25_F2981', 'L1_S25_F2984', \n",
    "             'L1_S25_F2986', 'L1_S25_F2989', 'L1_S25_F2991', 'L1_S25_F2994', 'L3_S46_F4136', 'L3_S46_F4137', \n",
    "             'L3_S47_F4139', 'L3_S47_F4142', 'L3_S47_F4144', 'L3_S47_F4147', 'L3_S47_F4149', 'L3_S47_F4152', \n",
    "             'L3_S47_F4154', 'L3_S47_F4157', 'L3_S47_F4159', 'L3_S47_F4162', 'L3_S47_F4164', 'L3_S47_F4167', \n",
    "             'L3_S47_F4169', 'L3_S47_F4172', 'L3_S47_F4174', 'L3_S47_F4177', 'L3_S47_F4179', 'L3_S47_F4182', \n",
    "             'L3_S47_F4184', 'L3_S47_F4187', 'L3_S47_F4189', 'L3_S47_F4192']\n",
    "\n",
    "\n",
    "for drop in drop_list:\n",
    "    train_categorical = train_categorical.drop(drop)\n",
    "\n",
    "train_categorical = train_categorical.na.fill('None')\n",
    "\n",
    "# This totally feels messy, but there does not seem to be a way to convert multiple categorical strings into one hots\n",
    "# from the ml pipeline framework.  Hopefully that is corrected in the future.\n",
    "ignore = ['Id', 'Response']\n",
    "categorical_columns = [x for x in train_categorical.columns if x not in ignore]\n",
    "\n",
    "indexed_df = train_categorical\n",
    "encoded_df = train_categorical\n",
    "\n",
    "for col in categorical_columns:\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=(col+\"_indexed\")).fit(encoded_df)\n",
    "    indexed_df = indexer.transform(encoded_df)\n",
    "    encoded_df = OneHotEncoder(inputCol=(col+\"_indexed\"), outputCol=(col+\"_vector\")).transform(indexed_df)\n",
    "    encoded_df = encoded_df.drop(col).drop(col+\"_indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoded_df.first().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load just like before\n",
    "source_test = source_dir + settings['test_numeric_file']\n",
    "data_test = sqlContext.read.csv(source_test, header = \"true\", inferSchema = \"true\")\n",
    "\n",
    "# And set null data to zero\n",
    "data_test = data_test.na.fill(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
