{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Bosch Production Line Challenge Model\n",
    "## Summary\n",
    "This is a [TensorFlow](https://www.tensorflow.org) model, built on a [Spark](https://spark.apache.org) framework, to attempt to solve the [Bosch Production Line Performance Challenge](https://www.kaggle.com/c/bosch-production-line-performance) on [Kaggle](https://www.kaggle.com).  This project was begun by Thomas Hughes on November 24, 2016, after the competition was completed.  It should be considered a test of effectiveness of technology platforms.\n",
    "\n",
    "## Notes on Execution\n",
    "Since this Notebook is designed to run with Spark, it must be running with the PySpark interpreter.  This can be done mostly automatically if you launch the notebook using the script 'pyspark-notebook' that is available in the github repository along with the notebook.  PySpark will need to be installed and properly configured, and you may need to update the script to your local copy of PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Bosch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notice!  Do not run this block more than once per kernel startup\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# sc is the SparkContext provided by the pyspark interpreter.  That's why you don't see it initialized here.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Source directory for your data\n",
    "source_dir = '/Users/thughes/tmp/data/'\n",
    "\n",
    "# Import Bosch training numeric data\n",
    "source_numeric = source_dir + 'train_numeric.csv'\n",
    "main_df_numeric = (sqlContext\n",
    "                       .read\n",
    "                       .format('com.databricks.spark.csv')\n",
    "                       .options(header='true', inferSchema='true')\n",
    "                       .load(source_numeric))\n",
    "\n",
    "# Now the categorical data\n",
    "source_categorical = source_dir + 'train_categorical.csv'\n",
    "\n",
    "main_df_categorical = (sqlContext\n",
    "                       .read\n",
    "                       .format('com.databricks.spark.csv')\n",
    "                       .options(header='true', inferSchema='true')\n",
    "                       .load(source_categorical))\n",
    "\n",
    "\n",
    "\n",
    "# Separate the target variable out from the rest\n",
    "train_y = main_df_numeric.select('Response')\n",
    "train_X_numeric = main_df_numeric.drop('Response')\n",
    "train_X_categorical = main_df_categorical.drop('Response')\n",
    "train_X = train_X_numeric.join(train_X_categorical, 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3109\n"
     ]
    }
   ],
   "source": [
    "print len(train_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Scale Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Normalize Data on Logarithmic Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Drop Outliers in Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Impute missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Feature Selection; use Spark libraries if possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independent Component Analysis (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Engineer some independent components using ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Engineer some principal components using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance\n",
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Bosch test numeric data\n",
    "source_numeric_test = source_dir + 'test_numeric.csv'\n",
    "main_df_numeric_test = (sqlContext\n",
    "                       .read\n",
    "                       .format('com.databricks.spark.csv')\n",
    "                       .options(header='true', inferSchema='true')\n",
    "                       .load(source_numeric))\n",
    "\n",
    "# Now the categorical data\n",
    "source_categorical_test = source_dir + 'test_categorical.csv'\n",
    "\n",
    "main_df_categorical_test = (sqlContext\n",
    "                       .read\n",
    "                       .format('com.databricks.spark.csv')\n",
    "                       .options(header='true', inferSchema='true')\n",
    "                       .load(source_categorical))\n",
    "\n",
    "\n",
    "\n",
    "# Separate the target variable out from the rest\n",
    "test_y = main_df_numeric_test.select('Response')\n",
    "test_X_numeric = main_df_numeric_test.drop('Response')\n",
    "test_X_categorical = main_df_categorical_test.drop('Response')\n",
    "test_X = test_X_numeric.join(test_X_categorical, 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1183747\n"
     ]
    }
   ],
   "source": [
    "print train_X.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
