{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Bosch Production Line Challenge Model\n",
    "## Summary\n",
    "This is a machine learning model, built on a [Spark](https://spark.apache.org) framework, to attempt to solve the [Bosch Production Line Performance Challenge](https://www.kaggle.com/c/bosch-production-line-performance) on [Kaggle](https://www.kaggle.com).  This project was begun by Thomas Hughes on November 24, 2016, after the competition was completed.  It should be considered a test of effectiveness of technology platforms.\n",
    "\n",
    "## Notes on Execution\n",
    "Since this Notebook is designed to run with Spark, it must be running with the PySpark interpreter.  This can be done mostly automatically if you launch the notebook using the script 'pyspark-notebook' that is available in the github repository along with the notebook.  PySpark will need to be installed and properly configured, and you may need to update the script to your local copy of PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load File Locations, using Kaggle specifications\n",
    "import json\n",
    "\n",
    "with open('SETTINGS.json') as settings_file:\n",
    "    settings = json.load(settings_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Bosch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# sc is the SparkContext provided by the pyspark interpreter.  That's why you don't see it initialized here.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Source directory for your data\n",
    "source_dir = settings['source_dir']\n",
    "\n",
    "# Import Bosch training numeric data\n",
    "source_numeric = source_dir + settings['train_numeric_file']\n",
    "train_numeric = sqlContext.read.csv(source_numeric, header = \"true\", inferSchema = \"true\")\n",
    "\n",
    "# Fill missing values with 0.\n",
    "train_numeric = train_numeric.na.fill(0)\n",
    "\n",
    "# Now the categorical data\n",
    "#source_categorical = source_dir + 'train_categorical.csv'\n",
    "#train_categorical = sqlContext.read.csv(source_categorical, header=\"true\", inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "### Vectorize Feature Space for Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need to vectorize our features for MLLib\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Only vectorize the non-ID and non-Response columns\n",
    "ignore = ['Id', 'Response']\n",
    "numeric_columns = [x for x in train_numeric.columns if x not in ignore]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = numeric_columns,\n",
    "    outputCol = 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol = \"Response\", featuresCol = \"features\", maxIter = 10, maxDepth = 10, \n",
    "                    maxMemoryInMB = 1024, maxBins = 64)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages = [assembler, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(train_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance\n",
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load just like before\n",
    "source_test = source_dir + settings['test_numeric_file']\n",
    "data_test = sqlContext.read.csv(source_test, header = \"true\", inferSchema = \"true\")\n",
    "\n",
    "# And set null data to zero\n",
    "data_test = data_test.na.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "preds = model.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format and Export Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Collect the prediction from Spark\n",
    "predsGBT = preds.select(\"prediction\").rdd.map(lambda r: r[0]).collect()\n",
    "\n",
    "# Format to Kaggle Format\n",
    "sub = pd.read_csv(source_dir + settings['sample_submission.csv'])\n",
    "sub['Response'] = np.asarray(predsGBT).astype(int)\n",
    "sub.to_csv(source_dir + settings['final_submission_file'], index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission History\n",
    "\n",
    "* Submission 1: -\t\tThomas M Hughes\t0.13591\t-\tSun, 27 Nov 2016 23:03:33 (GBT)\n",
    "* Submission 2: -\t\tThomas M Hughes\t0.13591\t-\tMon, 28 Nov 2016 00:06:04 (GBT w/ Standard Scaler)\n",
    "* Submission 3: -\t\tThomas M Hughes\t0.15070\t-\tMon, 28 Nov 2016 01:27:14 (GBT w/ maxDepth=10, maxBins=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
